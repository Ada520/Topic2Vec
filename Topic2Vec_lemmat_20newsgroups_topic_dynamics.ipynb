{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC2VEC algorithm by using gensim and according to the second hint given by Gordon Mohr.  \n",
    "We used a unique LDA (with lemmatized token) and then perform a different topic2vec learning on each window (and partition) in which we split the entire dataset. This has been done to compare topic representations obtained from different subsamples. \n",
    "All the steps are parallelized.\n",
    "(https://groups.google.com/forum/#!topic/gensim/BVu5-pD6910)\n",
    "\n",
    "1. Vectorization of docs (already tokenized by performing lemmatization) by using CountVectorizer (with or without tfidf) \n",
    "2. Latent Dirichlet Allocation \n",
    "3. Topic2Vec in each windows obtained from each partition of the entire dataset (20 NewsGroups)   \n",
    "\n",
    "It saves:\n",
    "* file with all the parameters used for CountVectorizer and LDA, the LDA vocabulary with the most likely words of each topic, the number of epochs used when learning the topic2vec and the seed to randomize the dataset partition\n",
    "* the topic2vec model for each window and each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd; import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import codecs \n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import pyorient\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import string\n",
    "import re\n",
    "# random\n",
    "from random import shuffle, seed\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partition = 1\n",
    "n_window_t2v = 2\n",
    "random_seed_partition = 54\n",
    "\n",
    "n_epoch_t2v = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CountVectorizer_param = {\n",
    "    'encoding' : 'utf-8',\n",
    "    'analyzer' : unicode.split, \n",
    "    'strip_accents' : 'unicode',\n",
    "    'ngram_range' : (1,1), \n",
    "    'min_df' : 2,\n",
    "    'max_df' : 0.95\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_param = {\n",
    "    'n_topics':8, \n",
    "    'max_iter':10, \n",
    "    'learning_method':'batch', \n",
    "    'learning_offset':50.,\n",
    "    'evaluate_every':0, \n",
    "    'n_jobs':-1, \n",
    "    'random_state':10\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_docs = 11314\n",
    "dir_name = '/results/20NG_lemmatiz_win5_n_topics' + str (lda_param['n_topics']) + '_n_doc' + str(n_docs) + '_n_win' + str(n_window_t2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LOADING LEMMATIZED/TOKENIZED TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd =  os.getcwd() # Prints the working directory\n",
    "results_dir_path = cwd + dir_name\n",
    "\n",
    "if not os.path.exists(results_dir_path):\n",
    "    os.makedirs(results_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = open(cwd + '/results/lemmatized_text_n_docs' + str(n_docs) + '.pkl', 'r')\n",
    "tokenized_docs = pickle.load(output) #space of the parameters spanned with the grid search\n",
    "output.close()\n",
    "\n",
    "tokenized_text = [unicode(x[0]) for x in tokenized_docs['tokenized_docs'] if len(x[0])>0]\n",
    "cat_docs = [x[1] for x in tokenized_docs['tokenized_docs'] if len(x[0])>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA to find the topic most-associated with each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 From Strings to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(encoding = CountVectorizer_param['encoding'],\n",
    "                                analyzer = CountVectorizer_param['analyzer'],\n",
    "                                strip_accents = CountVectorizer_param['strip_accents'],\n",
    "                                ngram_range = CountVectorizer_param['ngram_range'], \n",
    "                                min_df = CountVectorizer_param['min_df'],\n",
    "                                max_df = CountVectorizer_param['max_df']).fit(tokenized_text)\n",
    "tf_docs = tf_vectorizer.transform(tokenized_text)\n",
    "print(\"fit vectorizer with lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TFIDF (active/deactivate following cell to perform/not perform TFIDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tfidf_vectorizer = TfidfTransformer(sublinear_tf=False, use_idf = True).fit(tf_docs)\n",
    "tf_docs = tfidf_vectorizer.transform(tf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_words_dict(model, feature_names, n_top_words):\n",
    "    top_words_per_topic = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_per_topic['Topic_#' + str(topic_idx) + ':'] = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return top_words_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_docs, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=lda_param['n_topics'],\n",
    "                                max_iter=lda_param['max_iter'], \n",
    "                                learning_method=lda_param['learning_method'],\n",
    "                                learning_offset=lda_param['learning_offset'],\n",
    "                                evaluate_every=lda_param['evaluate_every'],\n",
    "                                n_jobs=lda_param['n_jobs'],\n",
    "                                random_state=lda_param['random_state'])\n",
    "t0 = time()\n",
    "lda.fit(tf_docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words_per_topic = top_words_dict(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_topic_distr_LDA = lda.components_\n",
    "per_topic_distr_LDA.shape\n",
    "#per_topic_distr_LDA.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CountVectorizer_param['analyzer'] = (CountVectorizer_param['analyzer'].__name__)\n",
    "\n",
    "output = open(results_dir_path + '/CountVect_LDA_param.pkl', 'w')\n",
    "\n",
    "pickle.dump({'lda_param': lda_param,\n",
    "             'CountVectorizer_param': CountVectorizer_param,\n",
    "             'top_words_per_topic': top_words_per_topic,\n",
    "             'random_seed_partition': random_seed_partition,\n",
    "             'n_epoch_t2v': n_epoch_t2v}, output) #space of the parameters spanned with the grid search\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TOPIC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_p_topic = np.argmax(per_topic_distr_LDA, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_and_topic = zip(tf_feature_names, most_p_topic)\n",
    "\n",
    "word2topic_dict = {word : 'topic_' + np.array_str(topic) for word, topic in word_and_topic}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_doc_to_topic(tokenized_text, prefix, doc_id_number, word2topic_dict):\n",
    "    doc_to_topic_list = [prefix + '_' + str(doc_id_number)]\n",
    "    for word in tokenized_text:\n",
    "        if word in word2topic_dict.keys():\n",
    "            doc_to_topic_list.append(word2topic_dict[word])\n",
    "            \n",
    "    return doc_to_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence_training(object):\n",
    "    def __init__(self, word2topic_dict, tokenized_docs, cat_docs):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.tokenized_docs = tokenized_docs\n",
    "        self.cat_docs = cat_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for idx, doc_cat in enumerate(zip(self.tokenized_docs,self.cat_docs)):\n",
    "            words_doc = doc_cat[0].split()\n",
    "            tags_doc = map_doc_to_topic(words_doc, doc_cat[1], idx, word2topic_dict)\n",
    "            yield models.doc2vec.LabeledSentence(words = words_doc,\n",
    "                                                 tags = tags_doc)\n",
    "                \n",
    "    def to_array(self):\n",
    "        if 'self.sentences' not in locals():\n",
    "            self.sentences = []\n",
    "            for idx, doc_cat in enumerate(zip(self.tokenized_docs,self.cat_docs)):\n",
    "                words_doc = doc_cat[0].split()\n",
    "                tags_doc = map_doc_to_topic(words_doc, doc_cat[1], idx, word2topic_dict)\n",
    "                self.sentences.append(models.doc2vec.LabeledSentence(words = words_doc,\n",
    "                                      tags = tags_doc))\n",
    "        return self.sentences\n",
    "            \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LabeledLineSentence_to_array(tokenized_doc, cat_doc, idx_doc):\n",
    "    words_doc = tokenized_doc.split()\n",
    "    tags_doc = map_doc_to_topic(words_doc, cat_doc, idx_doc, word2topic_dict)\n",
    "    return models.doc2vec.LabeledSentence(words = words_doc, tags = tags_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(lst, n_window, random_seed):\n",
    "    seed(random_seed)\n",
    "    division = len(lst) / float(n_window) \n",
    "    shuffle(lst)\n",
    "    return [ lst[int(round(division * i)): int(round(division * (i + 1)))] for i in xrange(n_window) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of docs to obtain the input object suitable for the doc2vec, UNPARALLELIZED"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "it = LabeledLineSentence_training(word2topic_dict,tokenized_text,cat_docs)\n",
    "all_docs = it.to_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of docs to obtain the input object suitable for the doc2vec, PARALLELIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed  \n",
    "import multiprocessing\n",
    "t0 = time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "all_docs = Parallel(n_jobs=num_cores)(delayed(LabeledLineSentence_to_array)(doc[0],doc[1],idx) \n",
    "                                            for idx, doc in enumerate(zip(tokenized_text,cat_docs)))\n",
    "print(\"Labelled sentences done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MULTIPLE PARTITIONS\n",
    "# Partitions and Topic2Vec on each windows of each partition\n",
    "t0 = time()\n",
    "for i_partition in xrange(n_partition):\n",
    "    print ('partition: ' + str(i_partition))\n",
    "    random_seed_partition += 1\n",
    "    partitioned_docs = partition(all_docs, n_window_t2v, random_seed_partition)\n",
    "\n",
    "    for i_window in xrange(n_window_t2v):\n",
    "        print ('window: ' + str(i_window))\n",
    "        current_partition = partitioned_docs[i_window]\n",
    "        model = models.Doc2Vec(size=100, window=5, min_count=1, dm=1, dbow_words=1,\n",
    "                              workers=20, alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "        model.build_vocab(current_partition)\n",
    "        for epoch in xrange(n_epoch_t2v):\n",
    "            print ('epoch: ' + str(epoch))\n",
    "            shuffle(current_partition)\n",
    "            model.train(current_partition)\n",
    "            model.alpha -= 0.002 # decrease the learning rate\n",
    "            model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "        fname = results_dir_path + '/t2v_20NG_partSEED' + str(random_seed_partition) + '_win' + str(i_window) + '.model'\n",
    "        model.save(fname)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SINGLE PARTITIONS\n",
    "t0 = time()\n",
    "partitioned_docs = partition(all_docs, n_window_t2v, random_seed_partition)\n",
    "for i_window in xrange(n_window_t2v):\n",
    "    current_partition = partitioned_docs[i_window]\n",
    "    model = models.Doc2Vec(size=100, window=10, min_count=1, dm=1, dbow_words=1,\n",
    "                           workers=50, alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "    model.build_vocab(current_partition)\n",
    "    for epoch in xrange(10):\n",
    "        shuffle(current_partition)\n",
    "        model.train(current_partition)\n",
    "        model.alpha -= 0.002 # decrease the learning rate\n",
    "        model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "    fname = results_dir_path + '/t2v_20NG_partSEED' + str(random_seed_partition) + '_win' + str(i_window) + '.model'\n",
    "    model.save(fname)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
