{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC2VEC algorithm by using gensim and according to the second hint given by Gordon Mohr.  \n",
    "(https://groups.google.com/forum/#!topic/gensim/BVu5-pD6910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd; import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import codecs \n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import pyorient\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff_txtlen = 200\n",
    "\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTING DOCS FROM BIP DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = '2015/11/01'\n",
    "stop = '2015/11/30'\n",
    "n_doc_per_day = 10\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drange = pd.date_range(start=start,end=stop,freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the Postgre SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "# Connect to an existing database\n",
    "conn = psycopg2.connect(\"dbname=bip user=cgnal host='151.80.103.221' password=CGnal2015!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to organize document in a unique table with the following fields:\n",
    "* **domain** [domain as detected from the classification algorithm]\n",
    "* **topic** [topic as detected from the classification algorithm]\n",
    "* **sourceDomain** [domain associated with the source of the document, if any]\n",
    "* **sourceTopic** [topic associated with the source of the document, if any]\n",
    "* **sourceType** [kind of source of the document: RSSfeed, twitter, etc...]\n",
    "* **sourceName** [name of the source of the document]\n",
    "* **author** [author of the document]\n",
    "* **publishDay** [publication date of the document]\n",
    "* **publishDate** [publication date in milliseconds of the document]\n",
    "* **title** [title of the document]\n",
    "* **ID** [ID of the document]\n",
    "* **pk** [numeric ID of the document]\n",
    "* **link** [link to the webpage where the original document has been found]\n",
    "* **sourceTags** [tags associated with the document, if any]\n",
    "* **text** [text of the document encoded with the utf-8 format]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas.io.sql as pdsql\n",
    "def random_textsPG(publishday, n_doc, conn, seed):\n",
    "    \"\"\" Iterator over documents in a day from the PS database \n",
    "    \n",
    "    Iterate over all documents in the database:\n",
    "        - on a specified day \n",
    "\n",
    "    yielding one document at a time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    publishday : string\n",
    "        day of publication of the selected documents \n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    id_name : string\n",
    "        id of the doc\n",
    "    record : dictionary-like object\n",
    "        record from the db\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Open a cursor to perform database operations\n",
    "    cur = conn.cursor()\n",
    "    # Count the number of records in the selected day\n",
    "    cur.execute(\"select count(*) from inputdocument where publishday = \" + publishday)\n",
    "    record_number = cur.fetchone()\n",
    "    cur.close()\n",
    "\n",
    "    if record_number[0] < n_doc:\n",
    "        sys.exit('Not enough document in the day: %s' % publishday)\n",
    "        \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT setseed(%s)\" %str(seed))\n",
    "    the_frame = pdsql.read_frame(\"select * from inputdocument where publishday = %s order by random() limit %s\" % (publishday,n_doc),  conn)\n",
    "#    the_frame['text']=the_frame['text'].apply(lambda x: x.decode('utf-8').encode('utf-8'))\n",
    "#    the_frame['text']=the_frame['text'].apply(lambda x: codecs.decode(x, 'utf-8'))\n",
    "#    the_frame.text=the_frame.text.apply(lambda x: codecs.decode(x, 'utf-8'));\n",
    "    the_frame.text=the_frame.text.apply(lambda x: x.decode('utf-8'));\n",
    "#    the_frame['text'] = codecs.decode(buffer(str(the_frame['text']),0,len(the_frame['text'])), 'utf-8')\n",
    "    cur.close()\n",
    "    \n",
    "    return the_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:36: FutureWarning: read_frame is deprecated, use read_sql\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tot = []\n",
    "random.seed(SEED)\n",
    "for day in drange:\n",
    "    tot += [random_textsPG(day.strftime(\"%Y%m%d\"), n_doc_per_day, conn, random.randrange(-1,1))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.concat(tot);\n",
    "df.index = np.arange(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of the data (check for duplicates or empty texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for empty text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_norm=df[df.text.apply(lambda x: len(x)>cutoff_txtlen)];\n",
    "num_empty_doc = df.shape[0] - df_norm.shape[0]\n",
    "num_empty_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm2 = df_norm.drop_duplicates(subset = 'text') #NB: inplace vuol dire passato per riferimento, altrimenti fa una copy qundi si tratta proprio di un'altra area di memoria\n",
    "num_doc_notUnique = df_norm.shape[0] - df_norm2.shape[0]\n",
    "num_doc_notUnique\n",
    "n_samples = df_norm2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TOTAL NUMBER OF DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_docs = df_norm2.shape[0]\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA to find the topic most-associated with each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH Lemmatization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Return the casting of the original tag in a single\n",
    "# character which is accepted by the lemmatizer\n",
    "import nltk.corpus  # splits on punctuactions   \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import re\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    # I recognize the initial character of the word, identifying the type\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.reader.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.reader.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.reader.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.reader.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from nltk import word_tokenize, pos_tag        \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokenized_doc = word_tokenize(doc) # splits on punctuactions  \n",
    "        tagged_doc = pos_tag(tokenized_doc)\n",
    "        \n",
    "        lemmatized_doc = []\n",
    "        # Scan the (word, tag) tuples which are the elements of tagged_tweet1\n",
    "        for word, tag in tagged_doc:\n",
    "            ret_value = get_wordnet_pos(tag)\n",
    "            # If the function does not return None I provide the ret_value\n",
    "            if ret_value != None:\n",
    "                lemmatized_doc.append(self.wnl.lemmatize(word, get_wordnet_pos(tag)))\n",
    "            # If the function returns None I do not provide the ret_value\n",
    "            else:\n",
    "                lemmatized_doc.append(self.wnl.lemmatize(word))\n",
    "        nonumbers_nopunct_lemmatized_doc = [word for word in lemmatized_doc if re.search('[a-zA-Z]{2,}', word)]\n",
    "#        nonumbers_nopunct_lemmatized_doc = [word for word in nopunct_lemmatized_doc if not re.search('\\d+', word)]\n",
    "        lemmatized_doc_stopw = [word for word in nonumbers_nopunct_lemmatized_doc if word not in stop_words]\n",
    "        \n",
    "        return lemmatized_doc_stopw #[self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='utf-8', analyzer='word',\n",
    "                                stop_words=[\"'s\", \"fx\"], ngram_range = (1,1), min_df = 2).fit(df_norm2.text)\n",
    "print(\"fit vectorizer with lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITHOUT Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit vectorizer without lemmatization done in 0.159s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(encoding='utf-8', analyzer='word', stop_words='english',\n",
    "                                ngram_range = (1,1), min_df = 2, token_pattern = '[a-zA-Z]{2,}').fit(df_norm2.text)\n",
    "print(\"fit vectorizer without lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"The Boston Globe Tweet Share\\nJessica Rinaldi/Globe Staff/file 2015\\nA flag hung from a wheelchair during celebrations earlier this year for the 25th anniversary of the Americans with Disabilities Act in Boston.\\nBy Tom Harkin and Jay Ruderman \\xa0\\nOctober 30, 2015\\nThe Americans with Disabilities Act, passed 25 years ago, unleashed both significant policy and culture change in American society. Prior to this historic legislation, people with disabilities were often excluded from the economy, isolated from mainstream society, and forced to live lives of dependence. They were pitied, not respected.\\nUndeniably, we\\u2019ve made great strides in the past quarter-century on both the policy and social fronts. America is a far more inclusive society. More people are living independent and integrated lives.\\nAdvertisement\\nBut we need to face the fact that much work remains to be done, particularly in providing jobs for and empowering people with disabilities in the workplace. Participation in the workforce by people with disabilities is only one-third that of people without disabilities. On the jobs front, we have a ways to go.\\nThe good news is that integrating people with disabilities into the workplace is not only good for people with disabilities, it\\u2019s good for the companies that hire them and good for the economy. Just as we all pay the price of underutilizing people with disabilities in the workplace, we all benefit by bringing them in, providing meaningful work, and extending them, like all employees, the possibility of realizing their highest callings.\\nThis isn\\u2019t just about being fair or humane. Here\\u2019s how more inclusive employment practices benefit us all:\\n\\u201cAs a disabled woman who was 8 when the Americans with Disabilities Act was passed, I think that media coverage of the recent anniversary misrepresents the state of the world.\\u201d \\u2014 Meghan Schrader\\nFirst, the companies that hire people with disabilities get highly dedicated and productive workers. The American Society of Safety Engineers found that employees with disabilities had roughly half the turnover rate of the average employee. In addition, medical costs for employees with disabilities were 67 percent lower and time-off expenses 73 percent lower. A DuPont study of 811 employees showed that people with disabilities rated 90 percent better in job performance.\\nSecond, companies that hire and develop people with disabilities foster better work cultures that likely enhance productivity for all employees. More than half of the employees in Deloitte\\u2019s 2015 Global Human Capital Trends survey said their companies are doing little if anything to improve employee engagement \\u2013 which ranked as the number-one challenge facing companies.\\nPrograms to recruit and integrate employees with disabilities impact the work culture by giving those involved a tremendous sense of satisfaction. We\\u2019ve seen over and over again the contagious sense of accomplishment from employees who have been involved in successful recruitment and retention efforts for people with disabilities.\\nWhen a company institutes such a program, it demonstrates its inclusive values to its employees, giving the broader workforce a sense that they are working for a values-driven organization. And when companies see this success, they expand their understanding of the varied and unique contributions each employee can make, generating a more diverse and positive culture overall.\\nThird, when more people work and live financially independent lives, they bring value to the economy. Employing people with disabilities \\u2014 nearly 20 percent of our population \\u2014 offers the possibility of economic growth and renewal.\\nFor the next 25 years of the ADA to improve employment and economic self-sufficiency for people with disabilities we will need a commitment on the part of employers, elected officials, and the broader public much like the efforts that were made to pass and implement the ADA. We need the data to show that employment of people with disabilities is good for business and society. We need to highlight the employment best practices. We need to shine a light on employer success stories. And we need to not give up until the workforce participation rate for people with disabilities is the same as for those without disabilities\\nLet\\u2019s get started. The next 25 years is now.\\nTom Harkin, a former US senator from Iowa, introduced the Americans with Disabilities Act in the Senate. Jay Ruderman is president of the Ruderman Family Foundation, a leading disability inclusion organization based in Boston. The Foundation is honoring Senator Harkin for his lifetime of dedication to disability inclusion at the inaugural Ruderman Inclusion Summit, November 1-2 in Boston.\\nGet Today\\u2019s Headlines from the Globe in your inbox:\\nPlease enter a valid email address\\nThank you for subscribing to Today's Headlines!\\nLoading comments...\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_norm2.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_docs = tf_vectorizer.transform(df_norm2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tfidf_vectorizer = TfidfTransformer(sublinear_tf=False, use_idf = True).fit(tf_docs)\n",
    "tfidf_docs = tfidf_vectorizer.transform(tf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=287 and n_features=6622...\n",
      "done in 0.990s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf_docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "class people working work workers new food resigned office time disabilities history president connor justice guilty like business federal service\n",
      "Topic #1:\n",
      "new said company nuclear gif companies rs year growth leave drone just paid security uploading capital resigned like line years\n",
      "Topic #2:\n",
      "said security attacks new russian udemy video people threat turkey india turkish share like reports russia told image state syria\n",
      "Topic #3:\n",
      "said company million year billion market cent khan companies china new years investors investment percent business time just shares according\n",
      "Topic #4:\n",
      "le la les et des pour en une est par il plus qui ce email que du sur dans qu\n",
      "Topic #5:\n",
      "football bitcoin people like things new make time di just use way business users life work college technology information don\n",
      "Topic #6:\n",
      "amp com track img src height http width border alt ft video js foxnews wealth display si style gif stats\n",
      "Topic #7:\n",
      "said di year social company people facebook media new la time work women del years paid global crore data della\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6622)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_topic_distr_LDA = lda.components_\n",
    "per_topic_distr_LDA.shape\n",
    "#per_topic_distr_LDA.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TOPIC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_p_topic = np.argmax(per_topic_distr_LDA, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_and_topic = zip(tf_feature_names, most_p_topic)\n",
    "\n",
    "word2topic_dict = {word : 'topic_' + np.array_str(topic) for word, topic in word_and_topic}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    text = \"\".join([ch for ch in document if ch not in string.punctuation])\n",
    "    text_list = text.split()\n",
    "    normalized_text = [x.lower() for x in text_list]\n",
    "    # Define an empty list\n",
    "    nostopwords_text = []\n",
    "    # Scan the words\n",
    "    for word in normalized_text:\n",
    "        # Determine if the word is contained in the stop words list\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            # If the word is not contained I append it\n",
    "            nostopwords_text.append(word)\n",
    "    tokenized_text = [word for word in nostopwords_text if re.search('[a-zA-Z]{2,}', word)]\n",
    "            \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_doc_to_topic(tokenized_text, doc_id_number, word2topic_dict):\n",
    "    doc_to_topic_list = ['paragraph_' + str(doc_id_number)]\n",
    "    for word in tokenized_text:\n",
    "        if word in word2topic_dict.keys():\n",
    "            doc_to_topic_list.append(word2topic_dict[word])\n",
    "            \n",
    "    return doc_to_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, docs_list, word2topic_dict):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.docs_list = docs_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.docs_list):\n",
    "            words_doc=tokenizer(doc)\n",
    "            tags_doc = map_doc_to_topic(words_doc, idx, word2topic_dict)\n",
    "            yield models.doc2vec.LabeledSentence(words = words_doc,\n",
    "                                                 tags = tags_doc)\n",
    "    def sentences_perm(self):\n",
    "        shuffle(models.doc2vec.LabeledSentence)\n",
    "        return models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "it = LabeledLineSentence(df_norm2.text, word2topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = models.Doc2Vec(size=100, window=8, min_count=2, dm=1, dbow_words=1,\n",
    "                              workers=50, alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "model.build_vocab(it)\n",
    "for epoch in range(10):\n",
    "    model.train(it.sentences_perm())\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname =  os.getcwd() # Prints the working directory\n",
    "fname = fname + '/topic2vec_ndoc' + str(n_docs) + 'n_topic' + str(n_topics) + '.model'\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph_0': Doctag(offset=0, word_count=401, doc_count=1),\n",
       " 'paragraph_1': Doctag(offset=7, word_count=107, doc_count=1),\n",
       " 'paragraph_10': Doctag(offset=16, word_count=262, doc_count=1),\n",
       " 'paragraph_100': Doctag(offset=107, word_count=355, doc_count=1),\n",
       " 'paragraph_101': Doctag(offset=108, word_count=264, doc_count=1),\n",
       " 'paragraph_102': Doctag(offset=109, word_count=180, doc_count=1),\n",
       " 'paragraph_103': Doctag(offset=110, word_count=131, doc_count=1),\n",
       " 'paragraph_104': Doctag(offset=111, word_count=312, doc_count=1),\n",
       " 'paragraph_105': Doctag(offset=112, word_count=183, doc_count=1),\n",
       " 'paragraph_106': Doctag(offset=113, word_count=200, doc_count=1),\n",
       " 'paragraph_107': Doctag(offset=114, word_count=136, doc_count=1),\n",
       " 'paragraph_108': Doctag(offset=115, word_count=200, doc_count=1),\n",
       " 'paragraph_109': Doctag(offset=116, word_count=301, doc_count=1),\n",
       " 'paragraph_11': Doctag(offset=17, word_count=234, doc_count=1),\n",
       " 'paragraph_110': Doctag(offset=117, word_count=188, doc_count=1),\n",
       " 'paragraph_111': Doctag(offset=118, word_count=227, doc_count=1),\n",
       " 'paragraph_112': Doctag(offset=119, word_count=40, doc_count=1),\n",
       " 'paragraph_113': Doctag(offset=120, word_count=149, doc_count=1),\n",
       " 'paragraph_114': Doctag(offset=121, word_count=698, doc_count=1),\n",
       " 'paragraph_115': Doctag(offset=122, word_count=328, doc_count=1),\n",
       " 'paragraph_116': Doctag(offset=123, word_count=60, doc_count=1),\n",
       " 'paragraph_117': Doctag(offset=124, word_count=254, doc_count=1),\n",
       " 'paragraph_118': Doctag(offset=125, word_count=177, doc_count=1),\n",
       " 'paragraph_119': Doctag(offset=126, word_count=520, doc_count=1),\n",
       " 'paragraph_12': Doctag(offset=18, word_count=21, doc_count=1),\n",
       " 'paragraph_120': Doctag(offset=127, word_count=381, doc_count=1),\n",
       " 'paragraph_121': Doctag(offset=128, word_count=592, doc_count=1),\n",
       " 'paragraph_122': Doctag(offset=129, word_count=396, doc_count=1),\n",
       " 'paragraph_123': Doctag(offset=130, word_count=200, doc_count=1),\n",
       " 'paragraph_124': Doctag(offset=131, word_count=457, doc_count=1),\n",
       " 'paragraph_125': Doctag(offset=132, word_count=204, doc_count=1),\n",
       " 'paragraph_126': Doctag(offset=133, word_count=99, doc_count=1),\n",
       " 'paragraph_127': Doctag(offset=134, word_count=135, doc_count=1),\n",
       " 'paragraph_128': Doctag(offset=135, word_count=412, doc_count=1),\n",
       " 'paragraph_129': Doctag(offset=136, word_count=57, doc_count=1),\n",
       " 'paragraph_13': Doctag(offset=19, word_count=284, doc_count=1),\n",
       " 'paragraph_130': Doctag(offset=137, word_count=484, doc_count=1),\n",
       " 'paragraph_131': Doctag(offset=138, word_count=222, doc_count=1),\n",
       " 'paragraph_132': Doctag(offset=139, word_count=170, doc_count=1),\n",
       " 'paragraph_133': Doctag(offset=140, word_count=372, doc_count=1),\n",
       " 'paragraph_134': Doctag(offset=141, word_count=605, doc_count=1),\n",
       " 'paragraph_135': Doctag(offset=142, word_count=74, doc_count=1),\n",
       " 'paragraph_136': Doctag(offset=143, word_count=227, doc_count=1),\n",
       " 'paragraph_137': Doctag(offset=144, word_count=60, doc_count=1),\n",
       " 'paragraph_138': Doctag(offset=145, word_count=248, doc_count=1),\n",
       " 'paragraph_139': Doctag(offset=146, word_count=287, doc_count=1),\n",
       " 'paragraph_14': Doctag(offset=20, word_count=79, doc_count=1),\n",
       " 'paragraph_140': Doctag(offset=147, word_count=258, doc_count=1),\n",
       " 'paragraph_141': Doctag(offset=148, word_count=296, doc_count=1),\n",
       " 'paragraph_142': Doctag(offset=149, word_count=78, doc_count=1),\n",
       " 'paragraph_143': Doctag(offset=150, word_count=493, doc_count=1),\n",
       " 'paragraph_144': Doctag(offset=151, word_count=38, doc_count=1),\n",
       " 'paragraph_145': Doctag(offset=152, word_count=204, doc_count=1),\n",
       " 'paragraph_146': Doctag(offset=153, word_count=141, doc_count=1),\n",
       " 'paragraph_147': Doctag(offset=154, word_count=163, doc_count=1),\n",
       " 'paragraph_148': Doctag(offset=155, word_count=119, doc_count=1),\n",
       " 'paragraph_149': Doctag(offset=156, word_count=331, doc_count=1),\n",
       " 'paragraph_15': Doctag(offset=21, word_count=1719, doc_count=1),\n",
       " 'paragraph_150': Doctag(offset=157, word_count=32, doc_count=1),\n",
       " 'paragraph_151': Doctag(offset=158, word_count=226, doc_count=1),\n",
       " 'paragraph_152': Doctag(offset=159, word_count=96, doc_count=1),\n",
       " 'paragraph_153': Doctag(offset=160, word_count=59, doc_count=1),\n",
       " 'paragraph_154': Doctag(offset=161, word_count=219, doc_count=1),\n",
       " 'paragraph_155': Doctag(offset=162, word_count=543, doc_count=1),\n",
       " 'paragraph_156': Doctag(offset=163, word_count=418, doc_count=1),\n",
       " 'paragraph_157': Doctag(offset=164, word_count=2475, doc_count=1),\n",
       " 'paragraph_158': Doctag(offset=165, word_count=83, doc_count=1),\n",
       " 'paragraph_159': Doctag(offset=166, word_count=19, doc_count=1),\n",
       " 'paragraph_16': Doctag(offset=23, word_count=309, doc_count=1),\n",
       " 'paragraph_160': Doctag(offset=167, word_count=91, doc_count=1),\n",
       " 'paragraph_161': Doctag(offset=168, word_count=105, doc_count=1),\n",
       " 'paragraph_162': Doctag(offset=169, word_count=215, doc_count=1),\n",
       " 'paragraph_163': Doctag(offset=170, word_count=48, doc_count=1),\n",
       " 'paragraph_164': Doctag(offset=171, word_count=191, doc_count=1),\n",
       " 'paragraph_165': Doctag(offset=172, word_count=283, doc_count=1),\n",
       " 'paragraph_166': Doctag(offset=173, word_count=250, doc_count=1),\n",
       " 'paragraph_167': Doctag(offset=174, word_count=144, doc_count=1),\n",
       " 'paragraph_168': Doctag(offset=175, word_count=202, doc_count=1),\n",
       " 'paragraph_169': Doctag(offset=176, word_count=219, doc_count=1),\n",
       " 'paragraph_17': Doctag(offset=24, word_count=845, doc_count=1),\n",
       " 'paragraph_170': Doctag(offset=177, word_count=60, doc_count=1),\n",
       " 'paragraph_171': Doctag(offset=178, word_count=266, doc_count=1),\n",
       " 'paragraph_172': Doctag(offset=179, word_count=58, doc_count=1),\n",
       " 'paragraph_173': Doctag(offset=180, word_count=34, doc_count=1),\n",
       " 'paragraph_174': Doctag(offset=181, word_count=102, doc_count=1),\n",
       " 'paragraph_175': Doctag(offset=182, word_count=138, doc_count=1),\n",
       " 'paragraph_176': Doctag(offset=183, word_count=103, doc_count=1),\n",
       " 'paragraph_177': Doctag(offset=184, word_count=448, doc_count=1),\n",
       " 'paragraph_178': Doctag(offset=185, word_count=71, doc_count=1),\n",
       " 'paragraph_179': Doctag(offset=186, word_count=351, doc_count=1),\n",
       " 'paragraph_18': Doctag(offset=25, word_count=478, doc_count=1),\n",
       " 'paragraph_180': Doctag(offset=187, word_count=168, doc_count=1),\n",
       " 'paragraph_181': Doctag(offset=188, word_count=224, doc_count=1),\n",
       " 'paragraph_182': Doctag(offset=189, word_count=261, doc_count=1),\n",
       " 'paragraph_183': Doctag(offset=190, word_count=184, doc_count=1),\n",
       " 'paragraph_184': Doctag(offset=191, word_count=80, doc_count=1),\n",
       " 'paragraph_185': Doctag(offset=192, word_count=262, doc_count=1),\n",
       " 'paragraph_186': Doctag(offset=193, word_count=323, doc_count=1),\n",
       " 'paragraph_187': Doctag(offset=194, word_count=299, doc_count=1),\n",
       " 'paragraph_188': Doctag(offset=195, word_count=218, doc_count=1),\n",
       " 'paragraph_189': Doctag(offset=196, word_count=178, doc_count=1),\n",
       " 'paragraph_19': Doctag(offset=26, word_count=361, doc_count=1),\n",
       " 'paragraph_190': Doctag(offset=197, word_count=21, doc_count=1),\n",
       " 'paragraph_191': Doctag(offset=198, word_count=76, doc_count=1),\n",
       " 'paragraph_192': Doctag(offset=199, word_count=167, doc_count=1),\n",
       " 'paragraph_193': Doctag(offset=200, word_count=259, doc_count=1),\n",
       " 'paragraph_194': Doctag(offset=201, word_count=55, doc_count=1),\n",
       " 'paragraph_195': Doctag(offset=202, word_count=125, doc_count=1),\n",
       " 'paragraph_196': Doctag(offset=203, word_count=153, doc_count=1),\n",
       " 'paragraph_197': Doctag(offset=204, word_count=208, doc_count=1),\n",
       " 'paragraph_198': Doctag(offset=205, word_count=195, doc_count=1),\n",
       " 'paragraph_199': Doctag(offset=206, word_count=306, doc_count=1),\n",
       " 'paragraph_2': Doctag(offset=8, word_count=225, doc_count=1),\n",
       " 'paragraph_20': Doctag(offset=27, word_count=162, doc_count=1),\n",
       " 'paragraph_200': Doctag(offset=207, word_count=317, doc_count=1),\n",
       " 'paragraph_201': Doctag(offset=208, word_count=33, doc_count=1),\n",
       " 'paragraph_202': Doctag(offset=209, word_count=204, doc_count=1),\n",
       " 'paragraph_203': Doctag(offset=210, word_count=1344, doc_count=1),\n",
       " 'paragraph_204': Doctag(offset=211, word_count=275, doc_count=1),\n",
       " 'paragraph_205': Doctag(offset=212, word_count=120, doc_count=1),\n",
       " 'paragraph_206': Doctag(offset=213, word_count=272, doc_count=1),\n",
       " 'paragraph_207': Doctag(offset=214, word_count=271, doc_count=1),\n",
       " 'paragraph_208': Doctag(offset=215, word_count=335, doc_count=1),\n",
       " 'paragraph_209': Doctag(offset=216, word_count=427, doc_count=1),\n",
       " 'paragraph_21': Doctag(offset=28, word_count=826, doc_count=1),\n",
       " 'paragraph_210': Doctag(offset=217, word_count=352, doc_count=1),\n",
       " 'paragraph_211': Doctag(offset=218, word_count=255, doc_count=1),\n",
       " 'paragraph_212': Doctag(offset=219, word_count=27, doc_count=1),\n",
       " 'paragraph_213': Doctag(offset=220, word_count=116, doc_count=1),\n",
       " 'paragraph_214': Doctag(offset=221, word_count=125, doc_count=1),\n",
       " 'paragraph_215': Doctag(offset=222, word_count=217, doc_count=1),\n",
       " 'paragraph_216': Doctag(offset=223, word_count=153, doc_count=1),\n",
       " 'paragraph_217': Doctag(offset=224, word_count=453, doc_count=1),\n",
       " 'paragraph_218': Doctag(offset=225, word_count=312, doc_count=1),\n",
       " 'paragraph_219': Doctag(offset=226, word_count=215, doc_count=1),\n",
       " 'paragraph_22': Doctag(offset=29, word_count=655, doc_count=1),\n",
       " 'paragraph_220': Doctag(offset=227, word_count=288, doc_count=1),\n",
       " 'paragraph_221': Doctag(offset=228, word_count=233, doc_count=1),\n",
       " 'paragraph_222': Doctag(offset=229, word_count=301, doc_count=1),\n",
       " 'paragraph_223': Doctag(offset=230, word_count=253, doc_count=1),\n",
       " 'paragraph_224': Doctag(offset=231, word_count=346, doc_count=1),\n",
       " 'paragraph_225': Doctag(offset=232, word_count=100, doc_count=1),\n",
       " 'paragraph_226': Doctag(offset=233, word_count=115, doc_count=1),\n",
       " 'paragraph_227': Doctag(offset=234, word_count=52, doc_count=1),\n",
       " 'paragraph_228': Doctag(offset=235, word_count=153, doc_count=1),\n",
       " 'paragraph_229': Doctag(offset=236, word_count=86, doc_count=1),\n",
       " 'paragraph_23': Doctag(offset=30, word_count=301, doc_count=1),\n",
       " 'paragraph_230': Doctag(offset=237, word_count=203, doc_count=1),\n",
       " 'paragraph_231': Doctag(offset=238, word_count=57, doc_count=1),\n",
       " 'paragraph_232': Doctag(offset=239, word_count=158, doc_count=1),\n",
       " 'paragraph_233': Doctag(offset=240, word_count=182, doc_count=1),\n",
       " 'paragraph_234': Doctag(offset=241, word_count=65, doc_count=1),\n",
       " 'paragraph_235': Doctag(offset=242, word_count=339, doc_count=1),\n",
       " 'paragraph_236': Doctag(offset=243, word_count=468, doc_count=1),\n",
       " 'paragraph_237': Doctag(offset=244, word_count=355, doc_count=1),\n",
       " 'paragraph_238': Doctag(offset=245, word_count=237, doc_count=1),\n",
       " 'paragraph_239': Doctag(offset=246, word_count=241, doc_count=1),\n",
       " 'paragraph_24': Doctag(offset=31, word_count=334, doc_count=1),\n",
       " 'paragraph_240': Doctag(offset=247, word_count=202, doc_count=1),\n",
       " 'paragraph_241': Doctag(offset=248, word_count=167, doc_count=1),\n",
       " 'paragraph_242': Doctag(offset=249, word_count=331, doc_count=1),\n",
       " 'paragraph_243': Doctag(offset=250, word_count=185, doc_count=1),\n",
       " 'paragraph_244': Doctag(offset=251, word_count=800, doc_count=1),\n",
       " 'paragraph_245': Doctag(offset=252, word_count=78, doc_count=1),\n",
       " 'paragraph_246': Doctag(offset=253, word_count=337, doc_count=1),\n",
       " 'paragraph_247': Doctag(offset=254, word_count=209, doc_count=1),\n",
       " 'paragraph_248': Doctag(offset=255, word_count=378, doc_count=1),\n",
       " 'paragraph_249': Doctag(offset=256, word_count=407, doc_count=1),\n",
       " 'paragraph_25': Doctag(offset=32, word_count=72, doc_count=1),\n",
       " 'paragraph_250': Doctag(offset=257, word_count=174, doc_count=1),\n",
       " 'paragraph_251': Doctag(offset=258, word_count=127, doc_count=1),\n",
       " 'paragraph_252': Doctag(offset=259, word_count=267, doc_count=1),\n",
       " 'paragraph_253': Doctag(offset=260, word_count=22, doc_count=1),\n",
       " 'paragraph_254': Doctag(offset=261, word_count=19, doc_count=1),\n",
       " 'paragraph_255': Doctag(offset=262, word_count=227, doc_count=1),\n",
       " 'paragraph_256': Doctag(offset=263, word_count=202, doc_count=1),\n",
       " 'paragraph_257': Doctag(offset=264, word_count=273, doc_count=1),\n",
       " 'paragraph_258': Doctag(offset=265, word_count=196, doc_count=1),\n",
       " 'paragraph_259': Doctag(offset=266, word_count=195, doc_count=1),\n",
       " 'paragraph_26': Doctag(offset=33, word_count=135, doc_count=1),\n",
       " 'paragraph_260': Doctag(offset=267, word_count=1117, doc_count=1),\n",
       " 'paragraph_261': Doctag(offset=268, word_count=154, doc_count=1),\n",
       " 'paragraph_262': Doctag(offset=269, word_count=272, doc_count=1),\n",
       " 'paragraph_263': Doctag(offset=270, word_count=975, doc_count=1),\n",
       " 'paragraph_264': Doctag(offset=271, word_count=489, doc_count=1),\n",
       " 'paragraph_265': Doctag(offset=272, word_count=129, doc_count=1),\n",
       " 'paragraph_266': Doctag(offset=273, word_count=189, doc_count=1),\n",
       " 'paragraph_267': Doctag(offset=274, word_count=62, doc_count=1),\n",
       " 'paragraph_268': Doctag(offset=275, word_count=124, doc_count=1),\n",
       " 'paragraph_269': Doctag(offset=276, word_count=268, doc_count=1),\n",
       " 'paragraph_27': Doctag(offset=34, word_count=197, doc_count=1),\n",
       " 'paragraph_270': Doctag(offset=277, word_count=277, doc_count=1),\n",
       " 'paragraph_271': Doctag(offset=278, word_count=129, doc_count=1),\n",
       " 'paragraph_272': Doctag(offset=279, word_count=22, doc_count=1),\n",
       " 'paragraph_273': Doctag(offset=280, word_count=289, doc_count=1),\n",
       " 'paragraph_274': Doctag(offset=281, word_count=194, doc_count=1),\n",
       " 'paragraph_275': Doctag(offset=282, word_count=369, doc_count=1),\n",
       " 'paragraph_276': Doctag(offset=283, word_count=68, doc_count=1),\n",
       " 'paragraph_277': Doctag(offset=284, word_count=51, doc_count=1),\n",
       " 'paragraph_278': Doctag(offset=285, word_count=141, doc_count=1),\n",
       " 'paragraph_279': Doctag(offset=286, word_count=42, doc_count=1),\n",
       " 'paragraph_28': Doctag(offset=35, word_count=96, doc_count=1),\n",
       " 'paragraph_280': Doctag(offset=287, word_count=99, doc_count=1),\n",
       " 'paragraph_281': Doctag(offset=288, word_count=893, doc_count=1),\n",
       " 'paragraph_282': Doctag(offset=289, word_count=453, doc_count=1),\n",
       " 'paragraph_283': Doctag(offset=290, word_count=300, doc_count=1),\n",
       " 'paragraph_284': Doctag(offset=291, word_count=473, doc_count=1),\n",
       " 'paragraph_285': Doctag(offset=292, word_count=124, doc_count=1),\n",
       " 'paragraph_286': Doctag(offset=293, word_count=449, doc_count=1),\n",
       " 'paragraph_29': Doctag(offset=36, word_count=78, doc_count=1),\n",
       " 'paragraph_3': Doctag(offset=9, word_count=518, doc_count=1),\n",
       " 'paragraph_30': Doctag(offset=37, word_count=883, doc_count=1),\n",
       " 'paragraph_31': Doctag(offset=38, word_count=554, doc_count=1),\n",
       " 'paragraph_32': Doctag(offset=39, word_count=669, doc_count=1),\n",
       " 'paragraph_33': Doctag(offset=40, word_count=334, doc_count=1),\n",
       " 'paragraph_34': Doctag(offset=41, word_count=346, doc_count=1),\n",
       " 'paragraph_35': Doctag(offset=42, word_count=355, doc_count=1),\n",
       " 'paragraph_36': Doctag(offset=43, word_count=406, doc_count=1),\n",
       " 'paragraph_37': Doctag(offset=44, word_count=402, doc_count=1),\n",
       " 'paragraph_38': Doctag(offset=45, word_count=907, doc_count=1),\n",
       " 'paragraph_39': Doctag(offset=46, word_count=112, doc_count=1),\n",
       " 'paragraph_4': Doctag(offset=10, word_count=273, doc_count=1),\n",
       " 'paragraph_40': Doctag(offset=47, word_count=170, doc_count=1),\n",
       " 'paragraph_41': Doctag(offset=48, word_count=96, doc_count=1),\n",
       " 'paragraph_42': Doctag(offset=49, word_count=283, doc_count=1),\n",
       " 'paragraph_43': Doctag(offset=50, word_count=79, doc_count=1),\n",
       " 'paragraph_44': Doctag(offset=51, word_count=196, doc_count=1),\n",
       " 'paragraph_45': Doctag(offset=52, word_count=303, doc_count=1),\n",
       " 'paragraph_46': Doctag(offset=53, word_count=126, doc_count=1),\n",
       " 'paragraph_47': Doctag(offset=54, word_count=216, doc_count=1),\n",
       " 'paragraph_48': Doctag(offset=55, word_count=320, doc_count=1),\n",
       " 'paragraph_49': Doctag(offset=56, word_count=102, doc_count=1),\n",
       " 'paragraph_5': Doctag(offset=11, word_count=103, doc_count=1),\n",
       " 'paragraph_50': Doctag(offset=57, word_count=178, doc_count=1),\n",
       " 'paragraph_51': Doctag(offset=58, word_count=551, doc_count=1),\n",
       " 'paragraph_52': Doctag(offset=59, word_count=1803, doc_count=1),\n",
       " 'paragraph_53': Doctag(offset=60, word_count=303, doc_count=1),\n",
       " 'paragraph_54': Doctag(offset=61, word_count=71, doc_count=1),\n",
       " 'paragraph_55': Doctag(offset=62, word_count=717, doc_count=1),\n",
       " 'paragraph_56': Doctag(offset=63, word_count=176, doc_count=1),\n",
       " 'paragraph_57': Doctag(offset=64, word_count=166, doc_count=1),\n",
       " 'paragraph_58': Doctag(offset=65, word_count=278, doc_count=1),\n",
       " 'paragraph_59': Doctag(offset=66, word_count=367, doc_count=1),\n",
       " 'paragraph_6': Doctag(offset=12, word_count=62, doc_count=1),\n",
       " 'paragraph_60': Doctag(offset=67, word_count=246, doc_count=1),\n",
       " 'paragraph_61': Doctag(offset=68, word_count=83, doc_count=1),\n",
       " 'paragraph_62': Doctag(offset=69, word_count=96, doc_count=1),\n",
       " 'paragraph_63': Doctag(offset=70, word_count=2210, doc_count=1),\n",
       " 'paragraph_64': Doctag(offset=71, word_count=454, doc_count=1),\n",
       " 'paragraph_65': Doctag(offset=72, word_count=33, doc_count=1),\n",
       " 'paragraph_66': Doctag(offset=73, word_count=65, doc_count=1),\n",
       " 'paragraph_67': Doctag(offset=74, word_count=266, doc_count=1),\n",
       " 'paragraph_68': Doctag(offset=75, word_count=53, doc_count=1),\n",
       " 'paragraph_69': Doctag(offset=76, word_count=378, doc_count=1),\n",
       " 'paragraph_7': Doctag(offset=13, word_count=396, doc_count=1),\n",
       " 'paragraph_70': Doctag(offset=77, word_count=586, doc_count=1),\n",
       " 'paragraph_71': Doctag(offset=78, word_count=114, doc_count=1),\n",
       " 'paragraph_72': Doctag(offset=79, word_count=41, doc_count=1),\n",
       " 'paragraph_73': Doctag(offset=80, word_count=232, doc_count=1),\n",
       " 'paragraph_74': Doctag(offset=81, word_count=823, doc_count=1),\n",
       " 'paragraph_75': Doctag(offset=82, word_count=440, doc_count=1),\n",
       " 'paragraph_76': Doctag(offset=83, word_count=234, doc_count=1),\n",
       " 'paragraph_77': Doctag(offset=84, word_count=173, doc_count=1),\n",
       " 'paragraph_78': Doctag(offset=85, word_count=316, doc_count=1),\n",
       " 'paragraph_79': Doctag(offset=86, word_count=3749, doc_count=1),\n",
       " 'paragraph_8': Doctag(offset=14, word_count=169, doc_count=1),\n",
       " 'paragraph_80': Doctag(offset=87, word_count=78, doc_count=1),\n",
       " 'paragraph_81': Doctag(offset=88, word_count=308, doc_count=1),\n",
       " 'paragraph_82': Doctag(offset=89, word_count=159, doc_count=1),\n",
       " 'paragraph_83': Doctag(offset=90, word_count=106, doc_count=1),\n",
       " 'paragraph_84': Doctag(offset=91, word_count=161, doc_count=1),\n",
       " 'paragraph_85': Doctag(offset=92, word_count=342, doc_count=1),\n",
       " 'paragraph_86': Doctag(offset=93, word_count=518, doc_count=1),\n",
       " 'paragraph_87': Doctag(offset=94, word_count=155, doc_count=1),\n",
       " 'paragraph_88': Doctag(offset=95, word_count=193, doc_count=1),\n",
       " 'paragraph_89': Doctag(offset=96, word_count=98, doc_count=1),\n",
       " 'paragraph_9': Doctag(offset=15, word_count=329, doc_count=1),\n",
       " 'paragraph_90': Doctag(offset=97, word_count=192, doc_count=1),\n",
       " 'paragraph_91': Doctag(offset=98, word_count=314, doc_count=1),\n",
       " 'paragraph_92': Doctag(offset=99, word_count=373, doc_count=1),\n",
       " 'paragraph_93': Doctag(offset=100, word_count=220, doc_count=1),\n",
       " 'paragraph_94': Doctag(offset=101, word_count=153, doc_count=1),\n",
       " 'paragraph_95': Doctag(offset=102, word_count=663, doc_count=1),\n",
       " 'paragraph_96': Doctag(offset=103, word_count=410, doc_count=1),\n",
       " 'paragraph_97': Doctag(offset=104, word_count=863, doc_count=1),\n",
       " 'paragraph_98': Doctag(offset=105, word_count=279, doc_count=1),\n",
       " 'paragraph_99': Doctag(offset=106, word_count=1101, doc_count=1),\n",
       " 'topic_0': Doctag(offset=4, word_count=2294171, doc_count=2601),\n",
       " 'topic_2': Doctag(offset=3, word_count=2513067, doc_count=5669),\n",
       " 'topic_3': Doctag(offset=1, word_count=14716705, doc_count=31013),\n",
       " 'topic_4': Doctag(offset=6, word_count=12746574, doc_count=6345),\n",
       " 'topic_5': Doctag(offset=2, word_count=10750994, doc_count=19909),\n",
       " 'topic_6': Doctag(offset=5, word_count=23727, doc_count=101),\n",
       " 'topic_7': Doctag(offset=22, word_count=3642, doc_count=4)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_tag = model.docvecs.doctags\n",
    "paragraphs_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paragraphs_vector = model.docvecs.doctag_syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('topic_7', 0.8170634508132935),\n",
       " ('topic_4', 0.6798727512359619),\n",
       " ('paragraph_163', 0.6333011984825134),\n",
       " ('topic_0', 0.578722357749939),\n",
       " ('paragraph_253', 0.5344024300575256),\n",
       " ('paragraph_112', 0.524728536605835),\n",
       " ('paragraph_173', 0.39938196539878845),\n",
       " ('paragraph_150', 0.3670458197593689),\n",
       " ('paragraph_229', 0.35714077949523926),\n",
       " ('topic_6', 0.35614171624183655)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(positive = 'paragraph_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.035059338029615694"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.n_similarity(['topic_0', 'topic_2'], ['topic_3', 'topic_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39070922733057056"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.similarity('topic_0', 'topic_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
