{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC2VEC algorithm by using gensim and according to the second hint given by Gordon Mohr.  \n",
    "We used a unique LDA (with token not lemmatized) and then perform a different topic2vec learning on each window (and partition) in which we split the entire dataset. This has been done to compare topic representations obtained from different subsamples.\n",
    "(https://groups.google.com/forum/#!topic/gensim/BVu5-pD6910)\n",
    "\n",
    "\n",
    "1. Vectorization of docs by using CountVectorizer (with or without tfidf) with no lemmatization\n",
    "2. Latent Dirichlet Allocation \n",
    "3. Topic2Vec in each windows obtained from each partition of the entire dataset (20 NewsGroups)   \n",
    "\n",
    "It saves:\n",
    "* the topic2vec model for each window and each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd; import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import codecs \n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import pyorient\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import string\n",
    "import re\n",
    "# random\n",
    "from random import shuffle, seed\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTING DOCS FROM 20 NEWSGROUPS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partition = 2\n",
    "n_window_t2v = 2\n",
    "random_seed_partition = 33\n",
    "n_topics_LDA = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories, shuffle=True, random_state=42)\n",
    "cat_docs = map(lambda x: newsgroups_train.target_names[x], newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TOTAL NUMBER OF DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_docs = newsgroups_train.filenames.shape[0]\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd =  os.getcwd() # Prints the working directory\n",
    "results_dir_path = cwd + '/results/20NG_n_topics' + str (n_topics_LDA) +'_n_doc' + str(n_docs) + '_n_win' + str(n_window_t2v)\n",
    "\n",
    "if not os.path.exists(results_dir_path):\n",
    "    os.makedirs(results_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA to find the topic most-associated with each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITHOUT Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(encoding='utf-8', analyzer='word', stop_words='english',\n",
    "                                ngram_range = (1,1), max_df=0.95, min_df = 50, token_pattern = '[a-zA-Z]{2,}').fit(newsgroups_train.data)\n",
    "tf_docs = tf_vectorizer.transform(newsgroups_train.data)\n",
    "print(\"fit vectorizer without lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_stop_words = tf_vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TFIDF (active/deactivate following cell to perform/not perform TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfTransformer(sublinear_tf=False, use_idf = True).fit(tf_docs)\n",
    "tf_docs = tfidf_vectorizer.transform(tf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_docs, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics_LDA, max_iter=10, \n",
    "                                learning_method='batch', learning_offset=50.,\n",
    "                                evaluate_every=1, n_jobs=-1, random_state=1)\n",
    "t0 = time()\n",
    "lda.fit(tf_docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_topic_distr_LDA = lda.components_\n",
    "per_topic_distr_LDA.shape\n",
    "#per_topic_distr_LDA.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TOPIC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_p_topic = np.argmax(per_topic_distr_LDA, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_and_topic = zip(tf_feature_names, most_p_topic)\n",
    "\n",
    "word2topic_dict = {word : 'topic_' + np.array_str(topic) for word, topic in word_and_topic}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    \n",
    "    text = \"\".join([ch for ch in document if ch not in string.punctuation])\n",
    "    text_list = text.split()\n",
    "    normalized_text = [x.lower() for x in text_list]\n",
    "    # Define an empty list\n",
    "    nostopwords_text = []\n",
    "    # Scan the words\n",
    "    for word in normalized_text:\n",
    "        # Determine if the word is contained in the stop words list\n",
    "        if word not in (ENGLISH_STOP_WORDS and my_stop_words):\n",
    "            # If the word is not contained I append it\n",
    "            nostopwords_text.append(word)\n",
    "    tokenized_text = [word for word in nostopwords_text if re.search('[a-zA-Z]{2,}', word)]\n",
    "            \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_doc_to_topic(tokenized_text, prefix, doc_id_number, word2topic_dict):\n",
    "    doc_to_topic_list = [prefix + '_' + str(doc_id_number)]\n",
    "    for word in tokenized_text:\n",
    "        if word in word2topic_dict.keys():\n",
    "            doc_to_topic_list.append(word2topic_dict[word])\n",
    "            \n",
    "    return doc_to_topic_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, docs_list, word2topic_dict):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.docs_list = docs_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.docs_list):\n",
    "            words_doc=tokenizer(doc)\n",
    "            tags_doc = map_doc_to_topic(words_doc, idx, word2topic_dict)\n",
    "            yield models.doc2vec.LabeledSentence(words = words_doc,\n",
    "                                                 tags = tags_doc)\n",
    "            \n",
    "    def sentences_perm(self):\n",
    "        shuffle(models.doc2vec.LabeledSentence)\n",
    "        return models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence_training(object):\n",
    "    def __init__(self, word2topic_dict, docs, cat_docs):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.docs = docs\n",
    "        self.cat_docs = cat_docs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for idx, doc_cat in enumerate(zip(self.docs,self.cat_docs)):\n",
    "            words_doc=tokenizer(doc_cat[0])\n",
    "            tags_doc = map_doc_to_topic(words_doc, doc_cat[1], idx, word2topic_dict)\n",
    "            yield models.doc2vec.LabeledSentence(words = words_doc,\n",
    "                                                     tags = tags_doc)\n",
    "                \n",
    "    def to_array(self):\n",
    "        if 'self.sentences' not in locals():\n",
    "            self.sentences = []\n",
    "            for idx, doc_cat in enumerate(zip(self.docs,self.cat_docs)):\n",
    "                words_doc=tokenizer(doc_cat[0])\n",
    "                tags_doc = map_doc_to_topic(words_doc, doc_cat[1], idx, word2topic_dict)\n",
    "                self.sentences.append(models.doc2vec.LabeledSentence(words = words_doc,\n",
    "                                                     tags = tags_doc))\n",
    "        return self.sentences\n",
    "            \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(lst, n_window, random_seed):\n",
    "    seed(random_seed)\n",
    "    division = len(lst) / float(n_window) \n",
    "    shuffle(lst)\n",
    "    return [ lst[int(round(division * i)): int(round(division * (i + 1)))] for i in xrange(n_window) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of docs to obtain the input object suitable for the doc2vec, UNPARALLELIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "it = LabeledLineSentence_training(word2topic_dict,newsgroups_train.data,cat_docs)\n",
    "all_docs = it.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MULTIPLE PARTITIONS\n",
    "# Partitions and Topic2Vec on each windows of each partition\n",
    "t0 = time()\n",
    "for i_partition in xrange(n_partition):\n",
    "    random_seed_partition += 1\n",
    "    partitioned_docs = partition(all_docs, n_window_t2v, random_seed_partition)\n",
    "    \n",
    "    for i_window in xrange(n_window_t2v):\n",
    "        current_partition = partitioned_docs[i_window]\n",
    "        model = models.Doc2Vec(size=100, window=10, min_count=1, dm=1, dbow_words=1,\n",
    "                              workers=20, alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "    \n",
    "        model.build_vocab(current_partition)\n",
    "        for epoch in xrange(20):\n",
    "            shuffle(current_partition)\n",
    "            model.train(current_partition)\n",
    "            model.alpha -= 0.002 # decrease the learning rate\n",
    "            model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "        fname = results_dir_path + '/t2v_20NG_partSEED' + str(random_seed_partition) + '_win' + str(i_window) + '.model'\n",
    "        model.save(fname)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SINGLE PARTITIONS\n",
    "t0 = time()\n",
    "partitioned_docs = partition(all_docs, n_window_t2v, random_seed_partition)\n",
    "for i_window in xrange(n_window_t2v):\n",
    "    current_partition = partitioned_docs[i_window]\n",
    "    model = models.Doc2Vec(size=100, window=10, min_count=1, dm=1, dbow_words=1,\n",
    "                           workers=50, alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "    model.build_vocab(current_partition)\n",
    "    for epoch in xrange(10):\n",
    "        shuffle(current_partition)\n",
    "        model.train(current_partition)\n",
    "        model.alpha -= 0.002 # decrease the learning rate\n",
    "        model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "    fname = results_dir_path + '/t2v_20NG_partSEED' + str(random_seed_partition) + '_win' + str(i_window) + '.model'\n",
    "    model.save(fname)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
